{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mnist_tensorflow_privacy.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "XAVN6c8prKOL"
      },
      "source": [
        "##### Copyright 2019 The TensorFlow Authors.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "both",
        "colab_type": "code",
        "id": "SassPC7WQAUO",
        "colab": {}
      },
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MfBg1C5NB3X0"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/privacy/blob/master/tutorials/Classification_Privacy.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/privacy/blob/master/tutorials/Classification_Privacy.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PAQcs7hLv6pu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ef56gCUqrdVn",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 1.x\n",
        "except Exception:\n",
        "  pass\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "tf.compat.v1.logging.set_verbosity(tf.logging.ERROR)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RseeuA7veIHU",
        "colab": {}
      },
      "source": [
        "!pip install tensorflow_privacy\n",
        "\n",
        "from tensorflow_privacy.privacy.analysis import compute_dp_sgd_privacy\n",
        "from tensorflow_privacy.privacy.optimizers.dp_optimizer import DPGradientDescentGaussianOptimizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_1ML23FlueTr",
        "colab": {}
      },
      "source": [
        "train, test = tf.keras.datasets.mnist.load_data()\n",
        "train_data, train_labels = train\n",
        "test_data, test_labels = test\n",
        "\n",
        "train_data = np.array(train_data, dtype=np.float32) / 255\n",
        "test_data = np.array(test_data, dtype=np.float32) / 255\n",
        "\n",
        "train_data = train_data.reshape(train_data.shape[0], 28, 28, 1)\n",
        "test_data = test_data.reshape(test_data.shape[0], 28, 28, 1)\n",
        "\n",
        "train_labels = np.array(train_labels, dtype=np.int32)\n",
        "test_labels = np.array(test_labels, dtype=np.int32)\n",
        "\n",
        "train_labels = tf.keras.utils.to_categorical(train_labels, num_classes=10)\n",
        "test_labels = tf.keras.utils.to_categorical(test_labels, num_classes=10)\n",
        "\n",
        "assert train_data.min() == 0.\n",
        "assert train_data.max() == 1.\n",
        "assert test_data.min() == 0.\n",
        "assert test_data.max() == 1."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "E14tL1vUuTRV",
        "colab": {}
      },
      "source": [
        "epochs = 15\n",
        "batch_size = 250"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pVw_r2Mq7ntd",
        "colab": {}
      },
      "source": [
        "l2_norm_clips = [0.5, 1, 2, 4]\n",
        "noise_multipliers = [0.5, 1, 2, 4]\n",
        "num_microbatches = 250\n",
        "learning_rate = 0.25\n",
        "\n",
        "if batch_size % num_microbatches != 0:\n",
        "  raise ValueError('Batch size should be an integer multiple of the number of microbatches')\n",
        "\n",
        "truncate_proportions_list = [\n",
        "                        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
        "                        [1, 1, 1, 1, 1, 1, 1, 1, 0.8, 1],\n",
        "                        [1, 1, 1, 1, 1, 1, 1, 1, 0.6, 1],\n",
        "                        [1, 1, 1, 1, 1, 1, 1, 1, 0.4, 1],\n",
        "                        [1, 1, 1, 1, 1, 1, 1, 1, 0.2, 1]\n",
        "                        ]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "euxcuJ_4VgrC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F0LNuYxWTMRl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "start_at = len(os.listdir('/content/drive/My Drive/cs839004/non_dp/'))\n",
        "current = 0\n",
        "\n",
        "for truncate_proportion_index in range(len(truncate_proportions_list)):\n",
        "  if current < start_at:\n",
        "    current += 1\n",
        "    continue\n",
        "\n",
        "  truncate_proportions = truncate_proportions_list[truncate_proportion_index]\n",
        "\n",
        "  indices = [[] for i in range(len(truncate_proportions))]\n",
        "\n",
        "  for label, proportion in enumerate(truncate_proportions):\n",
        "    # Locate indices where train_label == label\n",
        "    indices[label] = np.argwhere(train_labels[:, label])\n",
        "    # Shuffle those indices\n",
        "    indices[label] = indices[label][np.random.permutation(len(indices[label]))]\n",
        "    # Truncate those indices by proportion\n",
        "    indices[label] = indices[label][0:int(len(indices[label]) * truncate_proportions[label])].squeeze()\n",
        "\n",
        "  indices = np.concatenate(indices)\n",
        "\n",
        "  # Shuffle\n",
        "  indices = indices[np.random.permutation(len(indices))]\n",
        "\n",
        "  # Truncate to integer multiple of batch size (due to https://github.com/tensorflow/privacy/issues/40)\n",
        "  indices = indices[:(len(indices) // batch_size * batch_size)]\n",
        "\n",
        "  # Filter the train_data and train_labels by those indices\n",
        "  print(train_data.shape)\n",
        "  truncated_train_data = train_data[indices]\n",
        "  print(truncated_train_data.shape)\n",
        "  truncated_train_labels = train_labels[indices]\n",
        "\n",
        "  for label in range(len(truncate_proportions)):\n",
        "    print('Number of training samples for label ' + str(label) + ': ' +\n",
        "          str(np.count_nonzero(truncated_train_labels[:, label])))\n",
        "  \n",
        "  # Split test_data into a list of test data with the same label\n",
        "\n",
        "  test_data_label = []\n",
        "\n",
        "  for label in range(len(truncate_proportions)):\n",
        "    indices = np.argwhere(test_labels[:, label]).squeeze()\n",
        "    test_data_label.append(test_data[indices, :])\n",
        "\n",
        "  print('truncate_proportion_index: ' + str(truncate_proportions_list[truncate_proportion_index][8]))\n",
        "\n",
        "  model = tf.keras.Sequential([\n",
        "      tf.keras.layers.Conv2D(32, 3,\n",
        "                            padding='same',\n",
        "                            activation='relu',\n",
        "                            input_shape=(28, 28, 1)),\n",
        "      tf.keras.layers.MaxPool2D(2, 1),\n",
        "      tf.keras.layers.Conv2D(64, 3,\n",
        "                            padding='valid',\n",
        "                            activation='relu'),\n",
        "      tf.keras.layers.MaxPool2D(2, 1),\n",
        "      tf.keras.layers.Flatten(),\n",
        "      tf.keras.layers.Dense(64, activation='relu'),\n",
        "      tf.keras.layers.Dense(10, activation='softmax')\n",
        "  ])\n",
        "\n",
        "  loss = tf.keras.losses.CategoricalCrossentropy(\n",
        "      from_logits=True, reduction=tf.losses.Reduction.NONE)\n",
        "\n",
        "  model.compile(optimizer='adam', loss=loss, metrics=['accuracy'])\n",
        "\n",
        "  model.fit(truncated_train_data, truncated_train_labels,\n",
        "            epochs=epochs,\n",
        "            validation_data=(test_data, test_labels),\n",
        "            batch_size=batch_size)\n",
        "\n",
        "  # Create the .tf file manually as expected by ERAN\n",
        "\n",
        "  with open(f'./drive/My Drive/cs839004/non_dp/mnist_{truncate_proportions_list[truncate_proportion_index][8]}.tf', 'w') as f:\n",
        "    for layer in model.layers:\n",
        "      if isinstance(layer, tf.keras.layers.Conv2D):\n",
        "        f.write('Conv2D\\n')\n",
        "        activation = tf.keras.activations.serialize(layer.activation)\n",
        "        if activation == 'relu':\n",
        "          f.write('ReLU')\n",
        "        # add more here\n",
        "        f.write(', ')\n",
        "        f.write('filters={0}, '.format(layer.filters))\n",
        "        f.write('stride={0}, '.format(list(layer.strides)))\n",
        "        f.write('kernel_size={0}, '.format(list(layer.kernel_size)))\n",
        "        f.write('input_shape={0}, '.format([i for i in layer.input_shape if i is not None]))\n",
        "        f.write('padding={0}\\n'.format(1 if layer.padding == 'same' else 0))\n",
        "        for w in layer.get_weights():\n",
        "          f.write(str(np.ndarray.tolist(w)))\n",
        "          f.write('\\n')\n",
        "      elif isinstance(layer, tf.keras.layers.MaxPool2D):\n",
        "        f.write('MaxPooling2D\\n')\n",
        "        f.write('pool_size={0}, '.format(list(layer.pool_size)))\n",
        "        f.write('stride={0}, '.format(list(layer.strides)))\n",
        "        f.write('input_shape={0}\\n'.format([i for i in layer.input_shape if i is not None]))\n",
        "      elif isinstance(layer, tf.keras.layers.Flatten):\n",
        "        pass # Flattened by Dense layer. See https://github.com/eth-sri/eran/blob/master/tf_verify/read_net_file.py#L117\n",
        "      elif isinstance(layer, tf.keras.layers.Dense):\n",
        "        activation = tf.keras.activations.serialize(layer.activation)\n",
        "        if activation == 'relu':\n",
        "          f.write('ReLU')\n",
        "        elif activation == 'softmax':\n",
        "          f.write('Affine') # TODO not strictly true\n",
        "        f.write('\\n')\n",
        "        for w in layer.get_weights():\n",
        "          f.write(str(np.ndarray.tolist(w.transpose())))\n",
        "          f.write('\\n')\n",
        "\n",
        "  for label, data in enumerate(test_data_label):\n",
        "    results = model.evaluate(data, np.broadcast_to(tf.keras.utils.to_categorical(label, num_classes=10), (len(data), 10)), batch_size=1, verbose=0)\n",
        "    print('Accuracy for label ' + str(label) + ': ' + str(results[1]) + '. Number of test samples: ' + str(len(data)))\n",
        "  \n",
        "  current += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oCOo8aOLmFta",
        "colab": {}
      },
      "source": [
        "start_at = len(os.listdir('/content/drive/My Drive/cs839004/dp/'))\n",
        "current = 0\n",
        "\n",
        "for truncate_proportion_index in range(len(truncate_proportions_list)):\n",
        "  for l2_norm_clip in l2_norm_clips:\n",
        "    for noise_multiplier in noise_multipliers:\n",
        "      if current < start_at:\n",
        "        current += 1\n",
        "        continue\n",
        "\n",
        "      truncate_proportions = truncate_proportions_list[truncate_proportion_index]\n",
        "\n",
        "      indices = [[] for i in range(len(truncate_proportions))]\n",
        "\n",
        "      for label, proportion in enumerate(truncate_proportions):\n",
        "        # Locate indices where train_label == label\n",
        "        indices[label] = np.argwhere(train_labels[:, label])\n",
        "        # Shuffle those indices\n",
        "        indices[label] = indices[label][np.random.permutation(len(indices[label]))]\n",
        "        # Truncate those indices by proportion\n",
        "        indices[label] = indices[label][0:int(len(indices[label]) * truncate_proportions[label])].squeeze()\n",
        "\n",
        "      indices = np.concatenate(indices)\n",
        "\n",
        "      # Shuffle\n",
        "      indices = indices[np.random.permutation(len(indices))]\n",
        "\n",
        "      # Truncate to integer multiple of batch size (due to https://github.com/tensorflow/privacy/issues/40)\n",
        "      indices = indices[:(len(indices) // batch_size * batch_size)]\n",
        "\n",
        "      # Filter the train_data and train_labels by those indices\n",
        "      print(train_data.shape)\n",
        "      truncated_train_data = train_data[indices]\n",
        "      print(truncated_train_data.shape)\n",
        "      truncated_train_labels = train_labels[indices]\n",
        "\n",
        "      for label in range(len(truncate_proportions)):\n",
        "        print('Number of training samples for label ' + str(label) + ': ' +\n",
        "              str(np.count_nonzero(truncated_train_labels[:, label])))\n",
        "      \n",
        "      # Split test_data into a list of test data with the same label\n",
        "\n",
        "      test_data_label = []\n",
        "\n",
        "      for label in range(len(truncate_proportions)):\n",
        "        indices = np.argwhere(test_labels[:, label]).squeeze()\n",
        "        test_data_label.append(test_data[indices, :])\n",
        "\n",
        "      print('l2_norm_clip: ' + str(l2_norm_clip))\n",
        "      print('noise_multiplier: ' + str(noise_multiplier))\n",
        "      print('truncate_proportion_index: ' + str(truncate_proportions_list[truncate_proportion_index][8]))\n",
        "\n",
        "      model = tf.keras.Sequential([\n",
        "          tf.keras.layers.Conv2D(32, 3,\n",
        "                                padding='same',\n",
        "                                activation='relu',\n",
        "                                input_shape=(28, 28, 1)),\n",
        "          tf.keras.layers.MaxPool2D(2, 1),\n",
        "          tf.keras.layers.Conv2D(64, 3,\n",
        "                                padding='valid',\n",
        "                                activation='relu'),\n",
        "          tf.keras.layers.MaxPool2D(2, 1),\n",
        "          tf.keras.layers.Flatten(),\n",
        "          tf.keras.layers.Dense(64, activation='relu'),\n",
        "          tf.keras.layers.Dense(10, activation='softmax')\n",
        "      ])\n",
        "\n",
        "      optimizer = DPGradientDescentGaussianOptimizer(\n",
        "          l2_norm_clip=l2_norm_clip,\n",
        "          noise_multiplier=noise_multiplier,\n",
        "          num_microbatches=num_microbatches,\n",
        "          learning_rate=learning_rate)\n",
        "\n",
        "      loss = tf.keras.losses.CategoricalCrossentropy(\n",
        "          from_logits=True, reduction=tf.losses.Reduction.NONE)\n",
        "\n",
        "      model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
        "\n",
        "      model.fit(truncated_train_data, truncated_train_labels,\n",
        "                epochs=epochs,\n",
        "                validation_data=(test_data, test_labels),\n",
        "                batch_size=batch_size)\n",
        "\n",
        "      epsilon, _ = compute_dp_sgd_privacy.compute_dp_sgd_privacy(n=len(truncated_train_data), batch_size=num_microbatches, noise_multiplier=noise_multiplier, epochs=epochs, delta=1e-5)\n",
        "\n",
        "      # Create the .tf file manually as expected by ERAN\n",
        "\n",
        "      with open(f'./drive/My Drive/cs839004/dp/mnist_{truncate_proportions_list[truncate_proportion_index][8]}_{l2_norm_clip}_{noise_multiplier}_{epsilon}.tf', 'w') as f:\n",
        "        for layer in model.layers:\n",
        "          if isinstance(layer, tf.keras.layers.Conv2D):\n",
        "            f.write('Conv2D\\n')\n",
        "            activation = tf.keras.activations.serialize(layer.activation)\n",
        "            if activation == 'relu':\n",
        "              f.write('ReLU')\n",
        "            # add more here\n",
        "            f.write(', ')\n",
        "            f.write('filters={0}, '.format(layer.filters))\n",
        "            f.write('stride={0}, '.format(list(layer.strides)))\n",
        "            f.write('kernel_size={0}, '.format(list(layer.kernel_size)))\n",
        "            f.write('input_shape={0}, '.format([i for i in layer.input_shape if i is not None]))\n",
        "            f.write('padding={0}\\n'.format(1 if layer.padding == 'same' else 0))\n",
        "            for w in layer.get_weights():\n",
        "              f.write(str(np.ndarray.tolist(w)))\n",
        "              f.write('\\n')\n",
        "          elif isinstance(layer, tf.keras.layers.MaxPool2D):\n",
        "            f.write('MaxPooling2D\\n')\n",
        "            f.write('pool_size={0}, '.format(list(layer.pool_size)))\n",
        "            f.write('stride={0}, '.format(list(layer.strides)))\n",
        "            f.write('input_shape={0}\\n'.format([i for i in layer.input_shape if i is not None]))\n",
        "          elif isinstance(layer, tf.keras.layers.Flatten):\n",
        "            pass # Flattened by Dense layer. See https://github.com/eth-sri/eran/blob/master/tf_verify/read_net_file.py#L117\n",
        "          elif isinstance(layer, tf.keras.layers.Dense):\n",
        "            activation = tf.keras.activations.serialize(layer.activation)\n",
        "            if activation == 'relu':\n",
        "              f.write('ReLU')\n",
        "            elif activation == 'softmax':\n",
        "              f.write('Affine') # TODO not strictly true\n",
        "            f.write('\\n')\n",
        "            for w in layer.get_weights():\n",
        "              f.write(str(np.ndarray.tolist(w.transpose())))\n",
        "              f.write('\\n')\n",
        "\n",
        "      for label, data in enumerate(test_data_label):\n",
        "        results = model.evaluate(data, np.broadcast_to(tf.keras.utils.to_categorical(label, num_classes=10), (len(data), 10)), batch_size=1, verbose=0)\n",
        "        print('Accuracy for label ' + str(label) + ': ' + str(results[1]) + '. Number of test samples: ' + str(len(data)))\n",
        "      \n",
        "      current += 1"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}